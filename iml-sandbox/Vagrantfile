# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'open3'

# Create a set of /24 networks under a single /16 subnet range
SUBNET_PREFIX = '10.73'.freeze

# Management network for admin comms
MGMT_NET_PFX = "#{SUBNET_PREFIX}.10".freeze

# Lustre / HPC network
LNET_PFX = "#{SUBNET_PREFIX}.20".freeze

# Subnet index used to create cross-over nets for each HA cluster pair
# This currently appears to be pointless as unmanaged networks don't
# get an IP assigned.
xnet_idx = 230

ISCI_IP = "#{SUBNET_PREFIX}.40.10".freeze

ISCI_IP2 = "#{SUBNET_PREFIX}.50.10".freeze

Vagrant.configure('2') do |config|
  config.vm.box = 'centos/7'

  config.vm.provider 'virtualbox' do |vbx|
    vbx.linked_clone = true
    vbx.memory = 1024
    vbx.cpus = 4
    vbx.customize ['modifyvm', :id, '--audio', 'none']
  end

  config.vm.provision 'deps', type: 'shell', inline: <<-SHELL
    yum install -y epel-release
    yum install -y jq htop
  SHELL

  system("ssh-keygen -t rsa -N '' -f id_rsa") unless File.exist?('id_rsa')

  config.vm.provision 'ssh', type: 'shell', inline: <<-SHELL
    mkdir -m 0700 -p /root/.ssh
    cp /vagrant/id_rsa /root/.ssh/.
    chmod 0600 /root/.ssh/id_rsa
    mkdir -m 0700 -p /root/.ssh
    [ -f /vagrant/id_rsa.pub ] && (awk -v pk=\"`cat /vagrant/id_rsa.pub`\" 'BEGIN{split(pk,s,\" \")} $2 == s[2] {m=1;exit}END{if (m==0)print pk}' /root/.ssh/authorized_keys ) >> /root/.ssh/authorized_keys
    chmod 0600 /root/.ssh/authorized_keys

    cat > /etc/ssh/ssh_config <<__EOF
    Host *
      StrictHostKeyChecking no
__EOF
  SHELL

  config.vm.define 'iscsi' do |iscsi|
    iscsi.vm.hostname = 'iscsi.local'

    fix_hostfile iscsi
    provision_iscsi_net iscsi, '10'

    iscsi.vm.provider 'virtualbox' do |vbx|
      name = get_vm_name("iscsi")
      create_iscsi_disks(vbx, name)
    end

    ost_commands = ('d'..'z')
                   .take(20)
                   .flat_map.with_index do |x, i|
      [
        "targetcli /backstores/block create ost#{i + 1} /dev/sd#{x}",
        "targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:oss/tpg1/luns/ create /backstores/block/ost#{i + 1}"
      ]
    end
                   .join "\n"

    iscsi.vm.provision 'bootstrap', type: 'shell', inline: <<-SHELL
      yum -y install targetcli lsscsi
      targetcli /backstores/block create mgt1 /dev/sdb
      targetcli /backstores/block create mdt1 /dev/sdc
      targetcli /iscsi set global auto_add_default_portal=false
      targetcli /iscsi create iqn.2015-01.com.whamcloud.lu:mds
      targetcli /iscsi create iqn.2015-01.com.whamcloud.lu:oss
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/luns/ create /backstores/block/mgt1
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/luns/ create /backstores/block/mdt1
      #{ost_commands}
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/portals/ create #{ISCI_IP}
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:oss/tpg1/portals/ create #{ISCI_IP}
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/portals/ create #{ISCI_IP2}
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:oss/tpg1/portals/ create #{ISCI_IP2}
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/acls create iqn.2015-01.com.whamcloud:mds1
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:mds/tpg1/acls create iqn.2015-01.com.whamcloud:mds2
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:oss/tpg1/acls create iqn.2015-01.com.whamcloud:oss1
      targetcli /iscsi/iqn.2015-01.com.whamcloud.lu:oss/tpg1/acls create iqn.2015-01.com.whamcloud:oss2
      targetcli saveconfig
      systemctl enable target
    SHELL
  end

  #
  # Create an admin server for the cluster
  #
  config.vm.define 'adm', primary: true do |adm|
    adm.vm.hostname = 'adm.local'

    adm.vm.network 'forwarded_port', guest: 443, host: 8443

    # Admin / management network
    provision_mgmt_net adm, '10'

    provision_mdns adm, 'eth1'

    provision_fence_agents adm

    # Install IML onto the admin node
    # Using the devel repo from Copr
    adm.vm.provision 'install-iml-devel-copr',
                     type: 'shell', run: 'never',
                     inline: <<-SHELL
                      yum-config-manager --add-repo=https://raw.githubusercontent.com/whamcloud/integrated-manager-for-lustre/master/chroma_support.repo
                      yum install -y python2-iml-manager
                      chroma-config setup admin lustre localhost --no-dbspace-check
                     SHELL

    # Install IML onto the admin node
    # This requires you have the IML source tree available at
    # /integrated-manager-for-lustre
    adm.vm.provision 'install-iml-local',
                     type: 'shell', run: 'never',
                     inline: <<-SHELL
                       yum-config-manager --add-repo=https://raw.githubusercontent.com/whamcloud/integrated-manager-for-lustre/master/chroma_support.repo
                       yum install -y rpmdevtools git ed epel-release python-setuptools
                       cd /integrated-manager-for-lustre
                       make rpms
                       yum install -y /integrated-manager-for-lustre/_topdir/RPMS/noarch/python2-iml-manager-*
                       chroma-config setup admin lustre localhost --no-dbspace-check
                     SHELL

    adm.vm.provision 'install-stratagem-components',
                     type: 'shell', run: 'never',
                      inline: <<-SHELL
set -x
# Install action runner and stratagem service from the stratagem-devel copr repo
yum -y copr enable managerforlustre/stratagem-devel
yum -y install rust-iml-agent-comms rust-iml-stratagem rust-iml-action-runner
systemctl enable iml-agent-comms.service iml-action-runner.socket iml-stratagem.service

# fixup repolinks in lustre-server.repo and lustre-client.repo
sed -i 's|^baseurl=https:\/\/downloads\.whamcloud\.com\/public\/lustre\/.*|baseurl=https:\/\/build\.whamcloud\.com\/jobs-pub\/lustre-reviews\/configurations\/axis-arch\/x86_64\/axis-build_type\/client\/axis-distro\/el7\.6\/axis-ib_stack\/inkernel\/builds\/63298\/archive\/artifacts\/|' /usr/share/chroma-manager/lustre-client.repo
sed -i 's|^baseurl=https:\/\/downloads\.whamcloud\.com\/public\/lustre\/.*|baseurl=https:\/\/build\.whamcloud\.com\/jobs-pub\/lustre-reviews\/configurations\/axis-arch\/x86_64\/axis-build_type\/server\/axis-distro\/el7\.6\/axis-ib_stack\/inkernel\/builds\/63298\/archive\/artifacts\/|' /usr/share/chroma-manager/lustre-server.repo

# Delete an existing add of the managerforlustre-lnet-fix repo
sed -i '/managerforlustre-lnet-fix/,+9d' /usr/share/chroma-manager/base.repo

# Add the managerforlustre-lnet-fix repo to base.repo
echo "[managerforlustre-lnet-fix]
name=Copr repo for lnet-fix owned by managerforlustre
baseurl=https://copr-be.cloud.fedoraproject.org/results/managerforlustre/lnet-fix/epel-7-$basearch/
type=rpm-md
skip_if_unavailable=True
gpgcheck=1
gpgkey=https://copr-be.cloud.fedoraproject.org/results/managerforlustre/lnet-fix/pubkey.gpg
repo_gpgcheck=0
enabled=1
enabled_metadata=1" >> /usr/share/chroma-manager/base.repo

# Install stratagem repos
rm -rf /var/lib/chroma/repo/stratagem-server
rm /usr/share/chroma-manager/stratagem-server.repo
chroma-config repos install stratagem-server /vagrant/stratagem-server.tar.gz

# Add stratagem profile
echo '{
  "ui_name": "Stratagem Policy Engine Server",
  "ui_description": "A server running the Stratagem Policy Engine",
  "managed": true,
  "worker": false,
  "name": "stratagem_server",
  "initial_state": "managed",
  "ntp": true,
  "corosync": false,
  "corosync2": true,
  "pacemaker": true,
  "repolist": ["base", "lustre-server", "stratagem-server"],
  "packages": [
  "python2-iml-agent-management",
  "kernel-devel-lustre",
  "pcs",
  "fence-agents",
  "fence-agents-virsh",
  "lustre-resource-agents",
  "lustre-ldiskfs-zfs",
  "libyaml",
  "PyYAML",
  "python-dateutil",
  "python-flask",
  "python-gevent",
  "python-requests",
  "python-zmq",
  "python2-filelock",
  "protobuf-python",
  "pytz",
  "rsync",
  "zeromq3",
  "lipe",
  "lipe-pylustre"
  ]
}' > /tmp/stratagem-server.json
chroma-config profile register /tmp/stratagem-server.json
rm /tmp/stratagem-server.json
SHELL
             
  end

  #
  # Create the metadata servers (HA pair)
  #
  (1..2).each do |i|
    config.vm.define "mds#{i}" do |mds|
      mds.vm.hostname = "mds#{i}.local"

      mds.vm.provider 'virtualbox' do |vbx|
        vbx.name = "mds#{i}"
      end

      provision_lnet_net mds, "1#{i}"

      # Admin / management network
      provision_mgmt_net mds, "1#{i}"

      provision_iscsi_net mds, "1#{i}"

      # Private network to simulate crossover.
      # Used exclusively as additional cluster network
      mds.vm.network 'private_network',
                     ip: "#{SUBNET_PREFIX}.#{xnet_idx}.1#{i}",
                     netmask: '255.255.255.0',
                     auto_config: false,
                     virtualbox__intnet: "crossover-net#{xnet_idx}"

      # Increment the "crossover" subnet number so that
      # each HA pair has a unique "crossover" subnet
      xnet_idx += 1 if i.even?

      fix_hostfile mds

      provision_mdns mds, 'eth2'

      provision_iscsi_client mds, 'mds', i

      provision_mpath mds

      provision_fence_agents mds

      cleanup_storage_server mds

      configure_lustre_network mds

      install_lustre_zfs mds

      install_lustre_ldiskfs mds

      install_ldiskfs_no_iml mds

      install_stratagem_components mds

      pool_name = (i == 1 ? 'mgt' : 'mdt')

      config.vm.provision 'create-pools',
                          type: 'shell',
                          run: 'never',
                          inline: <<-SHELL
                            zpool create #{pool_name} -o multihost=on /dev/#{pool_name}
                          SHELL

      config.vm.provision 'add_udev_rule',
                          type: 'shell',
                          inline:
<<-SHELL
  cat > /etc/udev/rules.d/99-lustre-volume.rules <<EOF
  SUBSYSTEM=="block", ATTR{size}=="1048576", SYMLINK+="mgt"
  SUBSYSTEM=="block", ATTR{size}=="10485760", SYMLINK+="mdt"
EOF

  udevadm trigger --action=change --subsystem-match=block
SHELL

      if i == 1
        config.vm.provision 'create-zfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkfs.lustre --failover 10.73.20.12@tcp --mgs --backfstype=zfs mgs/mgt
                              mkdir -p /lustre/mgs
                              mount -t lustre mgs/mgt /lustre/mgs
                            SHELL

        config.vm.provision 'create-ldiskfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkdir -p /mnt/mgs
                              mkfs.lustre --mgs --reformat --servicenode=10.73.20.11@tcp --servicenode=10.73.20.12@tcp /dev/mgt
                              mount -t lustre /dev/mgt /mnt/mgs
                            SHELL

      else
        config.vm.provision 'create-zfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkfs.lustre --failover 10.73.20.11@tcp --mdt --backfstype=zfs --fsname=zfsmo --index=0 --mgsnode=10.73.20.11@tcp mds/mdt0
                              mkdir -p /lustre/zfsmo/mdt0
                              mount -t lustre mds/mdt0 /lustre/zfsmo/mdt0
                            SHELL

        config.vm.provision 'create-ldiskfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkdir -p /mnt/mdt
                              mkfs.lustre --mdt --reformat --servicenode=10.73.20.11@tcp --servicenode=10.73.20.12@tcp --index=0 --mgsnode=10.73.20.11@tcp --fsname=fs /dev/mdt
                              mount -t lustre /dev/mdt /mnt/mdt
                            SHELL
      end
    end
  end

  #
  # Create the object storage servers (OSS)
  # Servers are configured in HA pairs
  #
  (1..2).each do |i|
    config.vm.define "oss#{i}",
                     autostart: i <= 2 do |oss|

      oss.vm.hostname = "oss#{i}.local"

      oss.vm.provider 'virtualbox' do |vbx|
        vbx.name = "oss#{i}"
      end

      # Lustre / application network
      provision_lnet_net oss, "2#{i}"

      # Admin / management network
      provision_mgmt_net oss, "2#{i}"

      provision_iscsi_net oss, "2#{i}"

      # Private network to simulate crossover.
      # Used exclusively as additional cluster network
      oss.vm.network 'private_network',
                     ip: "#{SUBNET_PREFIX}.#{xnet_idx}.2#{i}",
                     netmask: '255.255.255.0',
                     auto_config: false,
                     virtualbox__intnet: "crossover-net#{xnet_idx}"

      # Increment the "crossover" subnet number so that
      # each HA pair has a unique "crossover" subnet
      xnet_idx += 1 if i.even?

      slice = i == 1 ? '0:10' : '10:20'

      fix_hostfile oss

      provision_mdns oss, 'eth2'

      provision_iscsi_client oss, 'oss', i

      provision_mpath oss

      provision_fence_agents oss

      cleanup_storage_server oss

      configure_lustre_network oss

      install_lustre_zfs oss

      install_lustre_ldiskfs oss

      install_ldiskfs_no_iml oss

      install_stratagem_components oss

      config.vm.provision 'create-pools',
                          type: 'shell',
                          run: 'never',
                          env: { 'device_query' => get_oss_block_device(slice, '') },
                          inline: <<-SHELL
                            block_device=$(eval $device_query)
                            zpool create oss#{i} -o multihost=on raidz2 $block_device
                          SHELL

      if i == 1
        config.vm.provision 'create-zfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkfs.lustre --failover 10.73.20.22@tcp --ost --backfstype=zfs --fsname=zfsmo --index=0 --mgsnode=10.73.20.11@tcp oss1/ost00
                              mkdir -p /lustre/zfsmo/ost00
                              mount -t lustre oss1/ost00 /lustre/zfsmo/ost00
                            SHELL

        config.vm.provision 'create-ldiskfs-fs',
                            type: 'shell',
                            run: 'never',
                            env: { 'device_query' => get_oss_block_device(slice, 0) },
                            inline: <<-SHELL
                              mkdir -p /mnt/ost#{i}
                              block_device=$(eval $device_query)
                              mkfs.lustre --ost --reformat --servicenode=10.73.20.21@tcp --servicenode=10.73.20.22@tcp --index=0 --mgsnode=10.73.20.11@tcp --fsname=fs $block_device
                              mount -t lustre $block_device /mnt/ost#{i}
                            SHELL
      else
        config.vm.provision 'create-zfs-fs',
                            type: 'shell',
                            run: 'never',
                            inline: <<-SHELL
                              mkfs.lustre --failover  10.73.20.21@tcp --ost --backfstype=zfs --fsname=zfsmo --index=1 --mgsnode=10.73.20.11@tcp oss2/ost01
                              mkdir -p /lustre/zfsmo/ost01
                              mount -t lustre oss2/ost01 /lustre/zfsmo/ost01
                            SHELL
        config.vm.provision 'create-ldiskfs-fs',
                            type: 'shell',
                            run: 'never',
                            env: { 'device_query' => get_oss_block_device(slice, 1) },
                            inline: <<-SHELL
                              mkdir -p /mnt/ost#{i}
                              block_device=$(eval $device_query)
                              mkfs.lustre --ost --reformat --servicenode=10.73.20.21@tcp --servicenode=10.73.20.22@tcp --index=1 --mgsnode=10.73.20.11@tcp --fsname=fs $block_device
                              mount -t lustre $block_device /mnt/ost#{i}
                            SHELL

      end
    end
  end

  # Create a set of compute nodes.
  # By default, only 2 compute nodes are created.
  # The configuration supports a maximum of 8 compute nodes.
  (1..8).each do |i|
    config.vm.define "c#{i}",
                     autostart: i <= 2 do |c|
      c.vm.hostname = "c#{i}.local"

      # Admin / management network
      provision_mgmt_net c, "3#{i}"

      # Lustre / application network
      provision_lnet_net c, "3#{i}"

      config.vm.provision 'install-lustre-client',
                          type: 'shell',
                          inline: <<-SHELL
                            yum-config-manager --add-repo https://downloads.whamcloud.com/public/lustre/lustre-2.10.5/el7/client/
                            yum-config-manager --save --setopt='*.gpgcheck=1' downloads.whamcloud.com_public_lustre_lustre-2.10.5_el7_client_
                            yum-config-manager --save --setopt='*.gpgcheck=0' downloads.whamcloud.com_public_lustre_lustre-2.10.5_el7_client_
                            yum -y install lustre-client
                          SHELL

      provision_mdns c, 'eth1'
    end
  end
end

def provision_iscsi_net(config, num)
  config.vm.network 'private_network',
                    ip: "#{SUBNET_PREFIX}.40.#{num}",
                    netmask: '255.255.255.0',
                    virtualbox__intnet: 'iscsi-net'

  config.vm.network 'private_network',
                    ip: "#{SUBNET_PREFIX}.50.#{num}",
                    netmask: '255.255.255.0',
                    virtualbox__intnet: 'iscsi-net'
end

def provision_lnet_net(config, num)
  config.vm.network 'private_network',
                    ip: "#{LNET_PFX}.#{num}",
                    netmask: '255.255.255.0',
                    virtualbox__intnet: 'lnet-net'
end

def provision_mgmt_net(config, num)
  config.vm.network 'private_network',
                    ip: "#{MGMT_NET_PFX}.#{num}",
                    netmask: '255.255.255.0',
                    name: 'vboxnet0'
end

def provision_mpath(config)
  config.vm.provision 'mpath', type: 'shell', inline: <<-SHELL
    yum -y install device-mapper-multipath
    cp /usr/share/doc/device-mapper-multipath-*/multipath.conf /etc/multipath.conf
    systemctl start multipathd.service
    systemctl enable multipathd.service
  SHELL
end

def provision_fence_agents(config)
  config.vm.provision 'fence-agents', type: 'shell', inline: <<-SHELL
    yum install -y epel-release
    yum install -y yum-plugin-copr
    yum -y copr enable managerforlustre/manager-for-lustre-devel
    yum install -y fence-agents-vbox
  SHELL
end

def provision_mdns(config, iface)
  config.vm.provision 'mdns', type: 'shell', inline: <<-SHELL
    yum install -y epel-release
    yum install -y avahi nss-mdns
    sed -i 's/^#allow-interfaces=.*/allow-interfaces=#{iface}/' /etc/avahi/avahi-daemon.conf
    systemctl restart network
    systemctl enable --now avahi-daemon.socket
    systemctl start avahi-daemon.service
    systemctl status avahi-daemon.service
  SHELL
end

def cleanup_storage_server(config)
  config.vm.provision 'cleanup', type: 'shell', run: 'never', inline: <<-SHELL
    yum autoremove -y chroma-agent
    rm -rf /etc/iml
    rm -rf /var/lib/{chroma,iml}
    rm -rf /etc/yum.repos.d/Intel-Lustre-Agent.repo
  SHELL
end

def provision_iscsi_client(config, name, idx)
  config.vm.provision 'iscsi-client', type: 'shell', inline: <<-SHELL
    yum -y install iscsi-initiator-utils lsscsi
    echo "InitiatorName=iqn.2015-01.com.whamcloud:#{name}#{idx}" > /etc/iscsi/initiatorname.iscsi
    iscsiadm --mode discoverydb --type sendtargets --portal #{ISCI_IP}:3260 --discover
    iscsiadm --mode node --targetname iqn.2015-01.com.whamcloud.lu:#{name} --portal #{ISCI_IP}:3260 -o update -n node.startup -v automatic
    iscsiadm --mode node --targetname iqn.2015-01.com.whamcloud.lu:#{name} --portal #{ISCI_IP}:3260 -o update -n node.conn[0].startup -v automatic
    iscsiadm --mode node --targetname iqn.2015-01.com.whamcloud.lu:#{name} --portal #{ISCI_IP2}:3260 -o update -n node.startup -v automatic
    iscsiadm --mode node --targetname iqn.2015-01.com.whamcloud.lu:#{name} --portal #{ISCI_IP2}:3260 -o update -n node.conn[0].startup -v automatic
    systemctl start iscsi
  SHELL
end

def fix_hostfile(config)
  config.vm.provision 'fix-hosts',
                      type: 'shell',
                      inline: "sed  -i '/^127.0.0.1\t#{config.vm.hostname}*/d' /etc/hosts"
end

def configure_lustre_network(config)
  config.vm.provision 'configure-lustre-network',
                      type: 'shell',
                      run: 'never',
                      inline: <<-SHELL
                        modprobe lnet
                        lnetctl lnet configure
                        lnetctl net add --net tcp0 --if eth1
                        lnetctl net show --net tcp > /etc/lnet.conf
                        systemctl enable lnet.service
                      SHELL
end

def install_lustre_zfs(config)
  config.vm.provision 'install-lustre-zfs', type: 'shell', run: 'never', inline: <<-SHELL
    yum clean all
    yum install -y lustre-zfs
    genhostid
  SHELL
end

def install_lustre_ldiskfs(config)
  config.vm.provision 'install-lustre-ldiskfs', type: 'shell', run: 'never', inline: <<-SHELL
    yum clean all
    yum install -y lustre-ldiskfs
  SHELL
end

def install_ldiskfs_no_iml(config)
  config.vm.provision 'install-ldiskfs-no-iml', type: 'shell', run: 'never', inline: <<-SHELL
    yum-config-manager --add-repo=https://downloads.whamcloud.com/public/lustre/latest-release/el7/server
    yum-config-manager --add-repo=https://downloads.whamcloud.com/public/e2fsprogs/latest/el7/
    yum-config-manager --save --setopt='*.gpgcheck=1' downloads.whamcloud.com_public_e2fsprogs_latest_el7_
    yum-config-manager --save --setopt='*.gpgcheck=0' downloads.whamcloud.com_public_e2fsprogs_latest_el7_
    yum-config-manager --save --setopt='*.gpgcheck=1' downloads.whamcloud.com_public_lustre_latest-release_el7_server
    yum-config-manager --save --setopt='*.gpgcheck=0' downloads.whamcloud.com_public_lustre_latest-release_el7_server
    yum install -y lustre kmod-lustre-osd-ldiskfs ntp
    systemctl enable --now ntpd
  SHELL
end

def get_oss_block_device(slice, idxOrEmpty)
  <<-SHELL
    echo '"Stream"' | socat - UNIX-CONNECT:/var/run/device-scanner.sock | jq -r '.blockDevices | to_entries | map(.value) |  map(select(.isMpath)) | map(.paths | .[2]) | .[#{slice}] | .[#{idxOrEmpty}]'
  SHELL
end

def get_vm_name(id)
  out, err = Open3.capture2e("VBoxManage list vms")
  raise out unless err.exitstatus === 0
  
  path = path = File.dirname(__FILE__).split('/').last
  name = out.split(/\n/)
     .select { |x| x.start_with? "\"#{path}_#{id}" }
     .map { |x| x.tr('"', '')}
     .map { |x| x.split(' ')[0].strip }
     .first

  name
end

# Checks if a scsi controller exists.
# This is used as a predicate to create controllers, as vagrant does not provide this
# functionality by default.
def controller_exists(name, controller_name)
  return false if name.nil?

  out, err = Open3.capture2e("VBoxManage showvminfo #{name}")
  raise out unless err.exitstatus === 0

  out.split(/\n/)
     .select { |x| x.start_with? 'Storage Controller Name' }
     .map { |x| x.split(':')[1].strip }
     .any? { |x| x == controller_name }
end

# Creates a SATA Controller and attaches 10 disks to it
def create_iscsi_disks(vbox, name)
  unless controller_exists(name, 'SATA Controller')
    vbox.customize ['storagectl', :id,
                    '--name', 'SATA Controller',
                    '--add', 'sata']
  end

  dir = "#{ENV['HOME']}/VirtualBox\ VMs/vdisks"
  Dir.mkdir dir unless File.directory?(dir)

  osts = (1..20).map { |x| ["OST#{x}", '5120'] }

  [
    %w[mgt 512],
    %w[mdt0 5120]
  ].concat(osts).each_with_index do |(name, size), i|
    file_to_disk = "#{dir}/#{name}.vdi"
    port = (i + 1).to_s

    unless File.exist?(file_to_disk)
      vbox.customize ['createmedium',
                     'disk',
                     '--filename',
                     file_to_disk,
                     '--size',
                     size,
                     '--format',
                     'VDI',
                     '--variant',
                     'fixed']
    end

    vbox.customize ['storageattach', :id,
                   '--storagectl', 'SATA Controller',
                   '--port', port,
                   '--type', 'hdd',
                   '--medium', file_to_disk,
                   '--device', '0']

    vbox.customize ['setextradata', :id,
                   "VBoxInternal/Devices/ahci/0/Config/Port#{port}/SerialNumber",
                   name.ljust(20, '0')]
  end
end

def install_stratagem_components(node)
  node.vm.provision 'install-stratagem-components', type: 'shell', run: 'never', inline: <<-SHELL
yum -y copr enable managerforlustre/stratagem-devel
yum install -y rust-iml-agent
systemctl enable rust-iml-agent.path
SHELL
end